# OpenLit Translator Implementation Progress

## Objective
Implement an OpenTelemetry Span Processor that translates traces generated by the OpenLit SDK into OpenTelemetry GenAI Semantic Convention compliant traces. The goal is to **replace** the original OpenLit trace with a new, compliant trace, ensuring no duplicate data is exported.

## Approach: "Replacement" Strategy
We are using a "Replacement" strategy which involves:
1.  **Interception:** Intercept the original span created by OpenLit (`openlit.otel.tracing`).
2.  **Mutation:** Modify the original span's attributes in-place (renaming to `gen_ai.*`, etc.) to ensure it holds the correct data during processing.
3.  **Synthetic Span Creation:** Create a new "Synthetic Span" (`opentelemetry.util.genai.handler`) using the data from the mutated original span. This new span adheres strictly to OTel GenAI SemConv.
4.  **Suppression:** Mark the original OpenLit span as `Unsampled` (`trace_flags = 0`) to prevent it from being exported, leaving only the synthetic span.

## Completed Steps
1.  **Processor Implementation (`openlit_span_processor.py`):**
    *   Created `OpenlitSpanProcessor` class inheriting from `SpanProcessor`.
    *   Implemented `on_end` to trigger transformation logic.
    *   Implemented `_mutate_span_if_needed` to apply attribute and name transformations to the original span.
    *   Implemented `_build_invocation` to construct the `LLMInvocation` object for the synthetic span.
    *   Implemented message reconstruction logic to convert OpenLit's flat attributes into structured OTel GenAI messages.

2.  **Registration & Ordering (`__init__.py`):**
    *   Implemented `enable_openlit_translator` to register the processor.
    *   Added logic to **force-insert** the `OpenlitSpanProcessor` at the *beginning* of the `_span_processors` list. This is critical for suppression: our processor must run *before* the `BatchSpanProcessor` (exporter) to successfully mark the span as unsampled before it gets queued.

3.  **Attribute Mapping:**
    *   Defined `_DEFAULT_ATTR_TRANSFORMATIONS` to map OpenLit attributes (e.g., `gen_ai.usage.cost`) to OTel GenAI attributes (`gen_ai.usage.total_cost`).

## Current Status & Failures

### 1. Duplicate Spans (Suppression Failure)
*   **Issue:** We currently see **two spans** for each operation:
    1.  The Synthetic Span (Correct Scope: `opentelemetry.util.genai.handler`).
    2.  The Original Span (OpenLit Scope: `openlit.otel.tracing`).
*   **Diagnosis:** The suppression logic (`span._context._trace_flags = 0`) is failing to stop the original span from being exported. This strongly suggests that the **Processor Ordering** fix in `__init__.py` is not working as expected in the user's environment. The `BatchSpanProcessor` is likely running *before* our processor, queuing the span before we can mark it as unsampled.

### 2. Missing Attribute (`gen_ai.usage.total_cost`)
*   **Issue:** The attribute `gen_ai.usage.total_cost` appears in the **Original Span** (Row 3) but is missing from the **Synthetic Span** (Row 2).
*   **Diagnosis:**
    *   The transformation rule `"gen_ai.usage.cost": "gen_ai.usage.total_cost"` exists.
    *   The original span shows the *renamed* attribute (`total_cost`), confirming that mutation occurred.
    *   However, `_build_invocation` (which creates the synthetic span) seems to be reading a version of attributes that *lacks* this specific key.
    *   This might be due to `ReadableSpan.attributes` returning a stale or cached copy of the attributes before the mutation was fully applied or propagated.
*   **Attempted Fix:** We attempted to access `span._attributes` directly to bypass potential caching, but reverted the hardcoded check per user request. We need a robust, generic way to ensure all mutated attributes are captured.

## Next Steps
1.  **Fix Attribute Access:** Modify `_build_invocation` to robustly access the most up-to-date attributes (likely via `_attributes` direct access) to ensure `gen_ai.usage.total_cost` is copied to the synthetic span.
2.  **Resolve Suppression:** Investigate why processor ordering is failing. If reliable ordering cannot be achieved, consider alternative strategies:
    *   **"Mutation Only + Scope Hack":** Abandon the synthetic span. Instead, mutate the original span and use internal APIs to overwrite its `InstrumentationScope` to match OTel GenAI. This guarantees zero duplicates without relying on suppression.

---

# Traceloop AgentInvocation Evaluation Issue - Progress Report

## Summary

We are attempting to run evaluations (toxicity, bias, answer_relevancy, hallucination) on **AgentInvocation** spans in addition to LLMInvocation spans. While LLMInvocation evaluations work correctly, AgentInvocation evaluations never execute.

## Root Cause: Traceloop SDK Agent Span Lifecycle Issue

The Traceloop SDK (`traceloop-sdk`) has a fundamental issue with how it manages agent span lifecycles:

### The Problem

1. **Agent spans start but never end** - When using `@agent` decorator from Traceloop SDK, the span's `on_start()` callback is triggered, but `on_end()` is **never called** before program exit.

2. **Span attributes set after creation** - The `traceloop.span.kind=agent` attribute is not available when `on_start()` is called. It's set later by Traceloop SDK, but by then it's too late.

3. **Program exits prematurely** - The workflow function (`joke_workflow`) appears to exit without completing all steps, and without triggering normal span lifecycle callbacks.

### Evidence

```
# Agent span starts and is tracked successfully:
[TL_PROCESSOR] on_start: span=joke_translation.agent, tl.span.kind=None
[TL_PROCESSOR] on_start: Tracking potential agent span for atexit: joke_translation.agent (id=14331673234348184202), count=1
[TL_PROCESSOR] on_start: VERIFIED - span_id 14331673234348184202 is in dict

# But on_end is NEVER called for the agent span!
# The next on_end we see is for openai.chat inside the agent, not the agent itself

# By the time force_flush is called, the dict is mysteriously empty:
[TL_PROCESSOR] force_flush called, pending_agent_spans=0, keys=[]
```

### What Works

- **LLMInvocation evaluations** - Work correctly because `openai.chat` spans DO trigger `on_end()`
- **Span tracking** - Our module-level dict correctly tracks agent spans in `on_start()`
- **Evaluation manager** - Correctly configured for both LLMInvocation and AgentInvocation

### What Doesn't Work

- **AgentInvocation evaluations** - Never execute because:
  1. Agent spans' `on_end()` is never called (Traceloop SDK issue)
  2. The module-level dict entries mysteriously disappear between `on_start()` and program exit
  3. Workarounds (atexit handlers, force_flush processing) find an empty dict

## Attempted Solutions

### 1. Process agent spans in `on_end()` ❌
- Agent spans never trigger `on_end()` - Traceloop SDK doesn't call it

### 2. Track spans in `on_start()`, process in `force_flush()` ❌
- Dict entries disappear between `on_start()` and `force_flush()`
- Same dict_id, but count goes from 1 to 0 without any explicit deletion

### 3. Register atexit handler ❌
- By atexit time, the dict is already empty

### 4. Module-level dict (avoid instance issues) ❌
- Verified dict is the same instance (same id)
- Entries still disappear

## Technical Details

### Files Modified
- `util/opentelemetry-util-genai-traceloop-translator/src/opentelemetry/util/genai/processor/traceloop_span_processor.py`
  - Added agent span tracking in `on_start()`
  - Added `_process_pending_agent_spans()` method
  - Added atexit handler registration
  - Modified `force_flush()` and `shutdown()` to process pending agents

### Environment
- `OTEL_INSTRUMENTATION_GENAI_EVALS_EVALUATORS="Deepeval(LLMInvocation(toxicity,bias,answer_relevancy,hallucination),AgentInvocation(toxicity,bias,answer_relevancy,hallucination))"`

### Traceloop SDK Version
- Using `traceloop-sdk` with `@agent`, `@task`, `@workflow`, `@tool` decorators

## Suspected Issues in Traceloop SDK

1. **Agent span lifecycle** - `@agent` decorated functions don't properly trigger OpenTelemetry span lifecycle callbacks (`on_end`)

2. **Possible garbage collection** - The span objects or references may be getting garbage collected before we can process them

3. **Thread safety** - Possible race condition between the main thread and background evaluation threads

## Recommendations

1. **File issue with Traceloop SDK** - Report that `@agent` spans don't trigger `on_end()` callbacks properly

2. **Alternative approach** - Consider intercepting at the Traceloop decorator level rather than the OpenTelemetry span processor level

3. **Direct Traceloop integration** - Work with Traceloop team to ensure proper span lifecycle for agent spans

## Current State

- **LLMInvocation evaluations**: ✅ Working
- **AgentInvocation evaluations**: ❌ Not working due to Traceloop SDK span lifecycle issues
- **Workflow evaluations**: ❌ Not attempted (likely same issue)

## Next Steps

1. Investigate Traceloop SDK source code for agent span handling
2. Consider filing a bug report with Traceloop
3. Explore alternative approaches that don't rely on `on_end()` callback
