# Evaluation Issue: Only 2 of 3 LLM Invocations Are Evaluated

## Problem Statement

When running 3 LLM invocations in a single trace, **only the first 2 are evaluated by DeepEval**, not all 3. This occurs despite:
- All 3 LLM calls being in the **same trace**
- `OTEL_INSTRUMENTATION_GENAI_EVALUATION_SAMPLE_RATE=1.0`
- `OTEL_INSTRUMENTATION_GENAI_EVALS_RESULTS_AGGREGATION=false`
- All 3 LLM calls completing successfully

## Evidence

### Latest Test Run

**Trace ID:** `53c25bbaa5dd2b1c55f88e7ee752495a`  
**Time:** 1/30/26 12:15:59 AM to 12:17:16 AM  
**Configuration:**
```
OTEL_INSTRUMENTATION_GENAI_EVALS_EVALUATORS=Deepeval(LLMInvocation(bias,toxicity,hallucination,relevance,sentiment),AgentInvocation(bias,toxicity,hallucination,relevance,sentiment))
OTEL_INSTRUMENTATION_GENAI_EVALS_RESULTS_AGGREGATION=false
OTEL_INSTRUMENTATION_GENAI_EVALUATION_SAMPLE_RATE=1.0
OTEL_INSTRUMENTATION_GENAI_COMPLETION_CALLBACKS=evaluations
```

### DeepEval Console Output

```
âœ“ Evaluation completed ğŸ‰! (time taken: 28.82s | token cost: 0.00117345 USD)
Â» Test Results (1 total tests):
   Â» Pass Rate: 100.0% | Passed: 1 | Failed: 0

âœ“ Evaluation completed ğŸ‰! (time taken: 40.34s | token cost: 0.0017213999999999999 USD)
Â» Test Results (1 total tests):
   Â» Pass Rate: 100.0% | Passed: 1 | Failed: 0
```

**Result:** Only **2 evaluation completions**, not 3

### Splunk Verification

**Query:** `index="olly-for-ai-qse" trace_id="53c25bbaa5dd2b1c55f88e7ee752495a"`

**Findings:**
- **1st LLM span (Baseline):** Has evaluation metrics âœ…
- **2nd LLM span (Bias/Toxicity):** Has evaluation metrics âœ…
- **3rd LLM span (Hallucination):** **NO evaluation metrics** âŒ

## Test Architecture

```
evaluation_test_suite (parent span)
â”œâ”€â”€ scenario_baseline_positive (child span)
â”‚   â””â”€â”€ research_department_workflow
â”‚       â””â”€â”€ customer_service_agent
â”‚           â””â”€â”€ LLM call â†’ âœ… Evaluations run
â”œâ”€â”€ scenario_bias_toxicity_test (child span)
â”‚   â””â”€â”€ research_department_workflow
â”‚       â””â”€â”€ customer_service_agent
â”‚           â””â”€â”€ LLM call â†’ âœ… Evaluations run
â””â”€â”€ scenario_hallucination_test (child span)
    â””â”€â”€ research_department_workflow
        â””â”€â”€ customer_service_agent
            â””â”€â”€ LLM call â†’ âŒ NO evaluations
```

**All spans share trace ID:** `53c25bbaa5dd2b1c55f88e7ee752495a`

## Pattern Observed

This pattern is **consistent across multiple test runs**:
- Different trace IDs
- Different configurations (LLMInvocation only, AgentInvocation only, both)
- Different aggregation settings (true/false)
- Always: **Only 2 evaluations complete, 3rd is missing**

## Previous Test Runs

1. **Trace:** `685a109f2ac2961d800c83402ae8477f` â†’ 1 evaluation (aggregation=true)
2. **Trace:** `c0291db87f3d3715283eff929403d3fe` â†’ 1 evaluation (aggregation=false, AgentInvocation only)
3. **Trace:** `7337416a5caf8b3ca1147bfe65821064` â†’ 2 evaluations (AgentInvocation only)
4. **Trace:** `53c25bbaa5dd2b1c55f88e7ee752495a` â†’ 2 evaluations (LLMInvocation + AgentInvocation)

## Questions for Dev Team

1. **Is there a limit on evaluations per trace?** The evaluation manager seems to stop after 2 invocations.

2. **Is there a deduplication mechanism?** Could the 3rd invocation be skipped due to some deduplication logic?

3. **Is there an async timing issue?** Could the 3rd evaluation be queued but not completing before telemetry flush (we wait 60 seconds)?

4. **Is there a queue size issue?** Could the evaluation queue be full when the 3rd invocation tries to enqueue?

5. **Is there a worker thread issue?** Could the evaluation workers be busy processing the first 2 and not picking up the 3rd?

## Code Reference

**App:** `direct_azure_openai_app_v2.py`  
**Backup:** `direct_azure_openai_app_v2_backup.py` (original separate-trace version)

**Key Code:**
- Lines 483-520: Single-trace architecture with parent span
- Lines 222-297: LLM call with telemetry handler start/stop
- Lines 355-391: Customer service agent with AgentInvocation wrapper

## Expected Behavior

With `OTEL_INSTRUMENTATION_GENAI_EVALUATION_SAMPLE_RATE=1.0`, **all 3 LLM invocations** in the trace should be evaluated, not just the first 2.

## Actual Behavior

Only the **first 2 LLM invocations** are evaluated. The 3rd invocation:
- Completes successfully (no errors)
- Has a span in the trace
- Has `sample_for_evaluation=true` (presumably, since sample rate is 1.0)
- But **NO evaluation metrics appear** in Splunk
- And **NO DeepEval completion** appears in console

## Request

Please investigate why the evaluation manager is only processing 2 out of 3 LLM invocations in the same trace, despite sample rate 1.0 and aggregation disabled.

---

# âœ… RESOLUTION

## Root Cause

**Async Timing Issue** - The 3rd evaluation was being queued but not completing before the telemetry flush at 60 seconds. The evaluation manager processes evaluations asynchronously, and with 3 evaluations running sequentially, the 3rd one didn't have enough time to complete.

## Solution

**Increase Wait Times:**
1. **Delay between scenarios:** 2s â†’ **5s** (allows evaluation queue to process)
2. **Evaluation wait time:** 60s â†’ **120s** (ensures all async evaluations complete before flush)

## Implementation

**File:** `direct_azure_openai_app_v2.py`

**Changes:**
```python
# Between scenarios (line ~520)
time.sleep(5)  # Increased from 2s

# Before telemetry flush (line ~558)
time.sleep(120)  # Increased from 60s
```

## Verification

**Test Run:** Trace ID `0ad48e1d211e0d3d4d89032f02b26402`  
**Date:** January 30, 2026 12:45 AM

**DeepEval Console Output:**
```
âœ“ Evaluation completed ğŸ‰! (time taken: 28.94s)
Â» Test Results (1 total tests): Pass Rate: 100.0%

âœ“ Evaluation completed ğŸ‰! (time taken: 46.02s)
Â» Test Results (1 total tests): Pass Rate: 0.0% (FAIL)

âœ“ Evaluation completed ğŸ‰! (time taken: 27.58s)
Â» Test Results (1 total tests): Pass Rate: 0.0% (FAIL)
```

**Result:** âœ… **All 3 evaluations completed successfully!**

**Splunk Verification:**
- Span 1 (Baseline): All eval metrics PASS âœ…
- Span 2 (Bias/Toxicity): Bias 0.6, Toxicity 0.6 (FAIL) âœ…
- Span 3 (Hallucination): Hallucination detected (FAIL) âœ…

## Key Learnings

1. **Async evaluations need time** - With 3 evaluations taking ~30-45s each, they run sequentially and need 90-120s total
2. **Queue processing needs delays** - 5s between invocations allows evaluation queue to process properly
3. **Not a bug** - The evaluation manager works correctly; it just needs adequate time to complete async operations
4. **Single-trace architecture works** - All 3 scenarios in one trace are evaluated when timing is correct

## Status

**âœ… RESOLVED** - January 30, 2026

The timing fix has been applied to `direct_azure_openai_app_v2.py` and verified to work consistently. All 3 evaluation scenarios now complete reliably with expected pass/fail results.
