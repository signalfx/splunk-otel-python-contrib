# ============================================================================
# OpenTelemetry GenAI Instrumentation Configuration Template
# ============================================================================
# Copy this file to .env and fill in your values
# Priority: Environment Variables > .env file > Application defaults

# ============================================================================
# REQUIRED: LLM Provider Configuration
# ============================================================================
# Choose ONE of the following:

# Option 1: Azure OpenAI (Recommended for Enterprise)
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_API_KEY=your-azure-api-key-here
# AZURE_OPENAI_DEPLOYMENT=gpt-4
# AZURE_OPENAI_API_VERSION=2024-08-01-preview

# Option 2: OpenAI (Public API)
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_MODEL_NAME=gpt-4o-mini

# ============================================================================
# REQUIRED: OpenTelemetry Service Configuration
# ============================================================================
OTEL_SERVICE_NAME=alpha-release-test
OTEL_RESOURCE_ATTRIBUTES_DEPLOYMENT_ENVIRONMENT=ai-test-rc0

# ============================================================================
# REQUIRED: OpenTelemetry Exporter Configuration
# ============================================================================
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
OTEL_EXPORTER_OTLP_PROTOCOL=grpc
OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE=DELTA
OTEL_LOGS_EXPORTER=otlp
OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true

# ============================================================================
# GenAI Instrumentation Configuration (DEFAULTS - Can Override)
# ============================================================================

# Semantic Conventions
OTEL_SEMCONV_STABILITY_OPT_IN=gen_ai_latest_experimental

# Content Capture
OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT=true
OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT_MODE=SPAN_AND_EVENT

# Emitters Configuration
# Options: span, metric, event
# Recommended: span_metric_event (for Splunk AI Details tab)
OTEL_INSTRUMENTATION_GENAI_EMITTERS=span_metric_event

# Evaluators Configuration
# Format: deepeval(LLMInvocation(metric1,metric2),AgentInvocation(metric3,metric4))
# Available metrics: bias, toxicity, hallucination, relevance, sentiment
# All 5 metrics for both LLM and Agent invocations:
OTEL_INSTRUMENTATION_GENAI_EVALS_EVALUATORS=deepeval(LLMInvocation(bias,toxicity,hallucination,relevance,sentiment),AgentInvocation(bias,toxicity,hallucination,relevance,sentiment))

# Evaluation Sampling (0.0 to 1.0)
OTEL_INSTRUMENTATION_GENAI_EVALUATION_SAMPLE_RATE=1.0

# Enable Evaluation Callbacks
OTEL_INSTRUMENTATION_GENAI_COMPLETION_CALLBACKS=evaluations

# ============================================================================
# DeepEval Judge Model Configuration (for LLM-as-Judge evaluations)
# ============================================================================
# DeepEval uses a separate LLM as "judge" to evaluate responses
# These are configured automatically by setup_environment.sh when Azure credentials are set

# For Azure OpenAI (RECOMMENDED):
# CRITICAL: DEEPEVAL_LLM_BASE_URL must be just the endpoint, NO /openai/deployments suffix!
# CORRECT: https://your-resource.openai.azure.com
# WRONG:   https://your-resource.openai.azure.com/openai/deployments
DEEPEVAL_LLM_BASE_URL=${AZURE_OPENAI_ENDPOINT}
DEEPEVAL_LLM_MODEL=${AZURE_OPENAI_DEPLOYMENT}
DEEPEVAL_LLM_PROVIDER=azure
DEEPEVAL_LLM_API_KEY=${AZURE_OPENAI_API_KEY}
AZURE_API_VERSION=${AZURE_OPENAI_API_VERSION}

# DeepEval telemetry and file system settings
DEEPEVAL_TELEMETRY_OPT_OUT=YES
DEEPEVAL_FILE_SYSTEM=READ_ONLY

# For OpenAI (alternative):
# DEEPEVAL_LLM_MODEL=gpt-4o-mini
# DEEPEVAL_LLM_API_KEY=${OPENAI_API_KEY}

# ============================================================================
# Debug Configuration
# ============================================================================
# Enable debug logging (set to "true" for verbose output)
OTEL_INSTRUMENTATION_GENAI_DEBUG=false

# LiteLLM Debug (for Deepeval troubleshooting)
# LITELLM_LOG=DEBUG

# ============================================================================
# NOTES
# ============================================================================
# 1. This template shows DEFAULT values used by the application
# 2. You can override any value by setting it in your environment or .env file
# 3. Configuration priority: ENV > .env > Application defaults
# 4. For production, use environment variables or secrets management
# 5. Never commit .env files with real credentials to version control
# 6. See README.md for detailed configuration documentation
