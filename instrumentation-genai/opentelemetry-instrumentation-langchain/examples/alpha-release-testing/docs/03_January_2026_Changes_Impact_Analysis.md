# January 2026 Changes & Test Plan Impact Analysis
## O11y for AI - GA Release (PI2)

**Version:** 1.0  
**Date:** January 12, 2026  
**Author:** Senior AI/QE Architect  
**Status:** Critical - Action Required

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [January 2026 Changes Analysis](#january-2026-changes-analysis)
3. [Critical Blockers & Resolutions](#critical-blockers--resolutions)
4. [Test Plan Impact Assessment](#test-plan-impact-assessment)
5. [Revised Test Strategy](#revised-test-strategy)
6. [Updated Test Case Mapping](#updated-test-case-mapping)
7. [Risk Mitigation Plan](#risk-mitigation-plan)
8. [Action Items](#action-items)
9. [Timeline Adjustments](#timeline-adjustments)
10. [Recommendations](#recommendations)

---

## 1. Executive Summary

### 1.1 Critical Findings

Based on analysis of daily huddles and updates from **December 2025 - January 2026**, several **critical changes** have been identified that directly impact the GA test plan:

| Change Category | Impact Level | Status | Action Required |
|----------------|--------------|--------|-----------------|
| **Session Tracking Removal** | ğŸ”´ **CRITICAL** | âœ… Resolved | Remove 3 test cases |
| **Evaluation Metric Naming** | ğŸŸ¡ **HIGH** | ğŸ”„ In Progress | Update validators |
| **Multi-Judge Backend** | ğŸŸ¡ **HIGH** | âœ… Implemented | Add Circuit tests |
| **AI Defense Multi-Agent** | ğŸŸ¢ **MEDIUM** | âœ… Available | Integrate example |
| **OpenAI v2 Refactoring** | ğŸŸ¢ **MEDIUM** | âœ… Merged | Update test data |
| **Traceloop Translator** | ğŸŸ¢ **MEDIUM** | âœ… Available | Validate backend |
| **CrewAI Instrumentation** | ğŸŸ¢ **LOW** | ğŸ”„ In Progress | Optional for PI3 |
| **LlamaIndex RAG** | ğŸŸ¢ **LOW** | ğŸ”„ In Progress | Optional for PI3 |

### 1.2 Test Plan Changes Summary

```
Original Test Plan (Dec 18, 2025):
â”œâ”€â”€ Total Test Cases: 33 (30 P0 + 3 Session)
â”œâ”€â”€ Session Tests: 3 (TC-PI2-SESSION-01, 02, 03)
â””â”€â”€ Evaluation Tests: 2 (OpenAI judge only)

Revised Test Plan (Jan 12, 2026):
â”œâ”€â”€ Total Test Cases: 30 P0 (Session tests REMOVED)
â”œâ”€â”€ Session Tests: 0 (Deferred to PI3)
â”œâ”€â”€ Evaluation Tests: 3 (Multi-judge: OpenAI + Circuit)
â””â”€â”€ New Tests: 3 (Traceloop, Trace Details Backend, Multi-Judge)

Net Change: -3 session tests + 3 new tests = 30 P0 tests (FINAL)
```

### 1.3 Impact on GA Timeline

```
Original GA Date: February 25, 2026 âœ… NO CHANGE
Testing Deadline: January 20, 2026 âœ… NO CHANGE

Risk Level: ğŸŸ¡ MEDIUM â†’ ğŸŸ¢ LOW
Reason: Session tests removal reduces scope, new tests are smaller
```

---

## 2. January 2026 Changes Analysis

### 2.1 Changes from Daily Huddles (Dec 2025 - Jan 2026)

#### **Change #1: Session Tracking Deferred to PI3** ğŸ”´ CRITICAL

**Source:** Daily huddles (Jan 8-9, 2026)

**Original Plan:**
```yaml
Session Support: IN SCOPE for PI2
Test Cases:
  - TC-PI2-SESSION-01: Session ID Propagation (P0)
  - TC-PI2-SESSION-02: Session List View (P0)
  - TC-PI2-SESSION-03: Session Detail View (P0)
Effort: 2.5 days
```

**Change Details:**
```
Blocker: Session semantic convention discussion ongoing
Decision: De-prioritize session support to PI3
Reason: 
  - Semantic convention not finalized
  - gen_ai.session.id attribute not ready
  - Session List/Detail UI dependent on backend
Impact: Remove 3 P0 test cases from PI2
```

**Evidence from Huddles:**
```
Jan 8, 2026:
"Multi-turn Session Support (de-prioritized to PI3 - semantic 
convention discussion ongoing)"

"Session ID attribute (gen_ai.session.id) - not in PI2"

"Session List View and Session Detail View UI (dependent on 
session support)"
```

**Test Plan Impact:**
- âŒ **REMOVE** TC-PI2-SESSION-01: Session ID Propagation
- âŒ **REMOVE** TC-PI2-SESSION-02: Session List View
- âŒ **REMOVE** TC-PI2-SESSION-03: Session Detail View
- âœ… **DEFER** to PI3 (Q2 2026)

---

#### **Change #2: Evaluation Metric Naming Convention** ğŸŸ¡ HIGH

**Source:** Daily huddle (Jan 9, 2026)

**Original Schema:**
```python
# Separate metrics per evaluation type
gen_ai.evaluation.bias
gen_ai.evaluation.toxicity
gen_ai.evaluation.hallucination
gen_ai.evaluation.relevance
```

**New Schema:**
```python
# Single metric with name attribute
gen_ai.evaluation.score
  - Attribute: gen_ai.evaluation.name=bias
  - Attribute: gen_ai.evaluation.name=toxicity
  - Attribute: gen_ai.evaluation.name=hallucination
  - Attribute: gen_ai.evaluation.name=relevance
```

**Rationale:**
```
Background: Currently we use separate metrics for each evaluation
Proposal: Use single metric like gen_ai.evaluation or 
          gen_ai.evaluation.score with different 
          gen_ai.evaluation.name=bias attributes

Upstream event schema: gen_ai.evaluation.result
Proposed: gen_ai.evaluation.score
```

**Test Plan Impact:**
- âš ï¸ **UPDATE** Metric validators to support new naming
- âš ï¸ **UPDATE** Test assertions to check `gen_ai.evaluation.score`
- âš ï¸ **UPDATE** Test data to use new attribute structure

**Code Changes Required:**
```python
# OLD validator (validators/metric_validator.py)
def validate_evaluation_metrics(metrics):
    assert 'gen_ai.evaluation.bias' in metrics
    assert 'gen_ai.evaluation.toxicity' in metrics

# NEW validator (validators/metric_validator.py)
def validate_evaluation_metrics(metrics):
    eval_metrics = [m for m in metrics if m['name'] == 'gen_ai.evaluation.score']
    eval_names = [m['attributes']['gen_ai.evaluation.name'] for m in eval_metrics]
    assert 'bias' in eval_names
    assert 'toxicity' in eval_names
```

---

#### **Change #3: Multi-Judge Backend (Circuit + OpenAI)** ğŸŸ¡ HIGH

**Source:** Daily huddles (Nov 25 - Dec 19, 2025)

**Original Plan:**
```yaml
Platform Evaluations:
  - Judge: OpenAI (gpt-4o-mini) only
  - Test Cases: 2
```

**New Implementation:**
```yaml
Platform Evaluations:
  - Judge 1: OpenAI (gpt-4o-mini)
  - Judge 2: Circuit (Cisco hosted)
  - Test Cases: 3 (added multi-judge comparison)
```

**Evidence from Huddles:**
```
Nov 25, 2025:
"Wrapping up circuit integration in evals and demo app"

Dec 19, 2025:
"feat(deepeval): Add Cisco CircuIT evaluation model for 
Deepeval integration"

Jan 9, 2026:
"Platform-Side Evaluations with multi-judge backends 
(OpenAI + Circuit)"
```

**Test Plan Impact:**
- âœ… **ADD** TC-PI2-EVAL-03: Circuit Judge Validation
- âš ï¸ **UPDATE** TC-PI2-EVAL-02: Compare OpenAI vs Circuit scores
- âš ï¸ **UPDATE** Configuration to support judge selection

**New Test Case:**
```python
@pytest.mark.p0
def test_circuit_judge_evaluation(config, apm_client):
    """
    TC-PI2-EVAL-03: Validate Circuit judge generates quality scores.
    
    Expected:
    - Circuit judge produces scores for toxicity, bias, hallucination
    - Scores are non-deterministic but within expected ranges
    - Comparison with OpenAI shows consistency (Â±10%)
    """
    # Configure Circuit judge
    set_evaluation_config({
        'llm_judge': 'circuit',
        'sampling_rate': 1.0
    })
    
    # Generate test data
    trace_id = trigger_evaluation_test()
    
    # Validate Circuit scores
    trace = apm_client.get_trace(trace_id)
    circuit_scores = extract_evaluation_scores(trace, judge='circuit')
    
    assert circuit_scores['toxicity'] >= 0.0
    assert circuit_scores['toxicity'] <= 1.0
```

---

#### **Change #4: AI Defense Multi-Agent Travel Planner** ğŸŸ¢ MEDIUM

**Source:** Merged commit (Dec 19, 2025)

**Change Details:**
```
Commit: 6c3487a
Author: Aditya Mehra
Date: Dec 19, 2025
Title: feat(aidefense): Add OpenTelemetry instrumentation for 
       Cisco AI Defense SDK (#108)

New Files:
â”œâ”€â”€ instrumentation-aidefense/examples/multi_agent_travel_planner/
â”‚   â”œâ”€â”€ README.md (Security demo with malicious request blocking)
â”‚   â”œâ”€â”€ main.py (Multi-agent workflow with AI Defense)
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ util/ (OAuth2 token manager)

Features:
- Multi-agent workflow (Flight, Hotel, Activity specialists)
- AI Defense security checks before each agent
- Malicious request blocking ("I want to learn how to make bombs")
- Security event ID tracking (gen_ai.security.event_id)
- Full observability with OpenTelemetry
```

**Test Plan Impact:**
- âœ… **INTEGRATE** AI Defense example into test suite
- âœ… **VALIDATE** Security event ID correlation
- âœ… **USE** as reference for TC-PI2-AIDEF-01, 02, 03

**Integration Approach:**
```python
# Use AI Defense example as test fixture
@pytest.fixture
def ai_defense_travel_planner():
    """Deploy AI Defense travel planner for testing."""
    # Deploy example app
    deploy_app('multi_agent_travel_planner')
    yield
    # Cleanup
    cleanup_app('multi_agent_travel_planner')

# Test against real example
def test_ai_defense_malicious_blocking(ai_defense_travel_planner, apm_client):
    """Validate AI Defense blocks malicious requests."""
    # Trigger malicious request
    response = trigger_malicious_activity()
    
    # Validate blocking
    assert response.status_code == 403
    assert 'BLOCKED' in response.text
```

---

#### **Change #5: OpenAI v2 TelemetryHandler Migration** ğŸŸ¢ MEDIUM

**Source:** Merged commit (Jan 8, 2026)

**Change Details:**
```
Commit: 1fdf7b5
Author: shuwpan
Date: Jan 8, 2026
Title: [refactor] Migrate chat completions to TelemetryHandler (#106)

Impact: 537 lines changed (422 insertions, 115 deletions)

Changes:
- Migrated from direct tracer.start_as_current_span() to TelemetryHandler
- Better span lifecycle management (start_llm/stop_llm/fail_llm)
- Built LLMInvocation objects for content events
- Fixed latent bug: properly await AsyncStream.close()
- Added defensive checks for None spans
```

**Test Plan Impact:**
- âš ï¸ **UPDATE** Test data to reflect new span structure
- âš ï¸ **VALIDATE** TelemetryHandler pattern in traces
- âš ï¸ **CHECK** Async stream handling in streaming tests

**Validation Required:**
```python
def test_telemetry_handler_pattern(apm_client, trace_id):
    """Validate new TelemetryHandler pattern."""
    trace = apm_client.get_trace(trace_id)
    
    # Check for TelemetryHandler attributes
    llm_span = find_span_by_operation(trace, 'chat')
    assert 'telemetry.handler.version' in llm_span['attributes']
    
    # Validate lifecycle events
    assert llm_span['status'] in ['ok', 'error']
```

---

#### **Change #6: Traceloop Translator Available** ğŸŸ¢ MEDIUM

**Source:** Merged commit (Dec 6, 2025)

**Change Details:**
```
Commit: c8a4db1
Author: Pavan Sudheendra
Date: Dec 6, 2025
Title: feat: initial commit of the openlit translator (#93)

New Package:
â”œâ”€â”€ util/opentelemetry-util-genai-openlit-translator/
â”‚   â”œâ”€â”€ src/opentelemetry/util/genai/openlit/
â”‚   â”œâ”€â”€ src/opentelemetry/util/genai/processor/
â”‚   â”‚   â”œâ”€â”€ content_normalizer.py
â”‚   â”‚   â”œâ”€â”€ message_reconstructor.py
â”‚   â”‚   â””â”€â”€ openlit_span_processor.py
â”‚   â””â”€â”€ tests/ (7 test files)

Purpose: Translates OpenLit telemetry format to OpenTelemetry format
```

**Test Plan Impact:**
- âœ… **ADD** TC-PI2-TRACELOOP-01: Traceloop Backend Validation
- âœ… **VALIDATE** Backend processes Traceloop spans correctly
- âœ… **ENSURE** All platform features work with Traceloop data

**New Test Case:**
```python
@pytest.mark.p0
def test_traceloop_backend_processing(apm_client):
    """
    TC-PI2-TRACELOOP-01: Validate Traceloop instrumentation data 
    processed correctly by backend.
    
    Expected:
    - Traceloop spans translated to OpenTelemetry format
    - All platform features work (evals, AI Defense, cost)
    - UI displays Traceloop data correctly
    """
    # Deploy Traceloop app
    deploy_traceloop_app()
    
    # Generate traffic
    trace_id = trigger_traceloop_workflow()
    
    # Validate backend processing
    trace = apm_client.get_trace(trace_id)
    assert trace is not None
    
    # Validate span structure
    assert len(trace['spans']) > 0
    validate_genai_schema(trace['spans'][0])
```

---

#### **Change #7: CrewAI Instrumentation** ğŸŸ¢ LOW (PI3 Scope)

**Source:** Daily huddles (Nov 25 - Dec 2, 2025)

**Change Details:**
```
Nov 25, 2025:
"Got a CrewAI application instrumented with 
splunk-otel-instrumentation-crewai and deployed to k8s cluster"

Dec 2, 2025:
"Working on initial CrewAI instrumentation PR"

Status: In Progress (PR #89)
Scope: PI3 (not GA blocking)
```

**Test Plan Impact:**
- â„¹ï¸ **NO IMPACT** on PI2 test plan
- â„¹ï¸ **DEFER** CrewAI tests to PI3
- â„¹ï¸ **OPTIONAL** validation if time permits

---

#### **Change #8: LlamaIndex RAG Instrumentation** ğŸŸ¢ LOW (PI3 Scope)

**Source:** Daily huddles (Nov 25 - Dec 17, 2025)

**Change Details:**
```
Nov 25, 2025:
"Creating PR for Llamaindex embedding instrumentation"
"Working on RAG instrumentation"

Dec 17, 2025:
"Will add traces and spans for the two open PRs to review"

Status: In Progress
Scope: PI3 (not GA blocking)
```

**Test Plan Impact:**
- â„¹ï¸ **NO IMPACT** on PI2 test plan
- â„¹ï¸ **DEFER** LlamaIndex tests to PI3
- â„¹ï¸ **USE** existing RAG test (TC-PI2-FOUNDATION-05) for now

---

## 3. Critical Blockers & Resolutions

### 3.1 Blocker #1: Evaluation Metric Naming

**Status:** ğŸ”„ **IN PROGRESS**

**Issue:**
```
Current: gen_ai.evaluation.bias (separate metrics)
Proposed: gen_ai.evaluation.score with gen_ai.evaluation.name=bias
Decision: Pending final approval
```

**Resolution:**
```
Action: Update validators to support BOTH formats during transition
Timeline: Week 4 (Jan 6-10, 2026)
Owner: QE Team
Risk: LOW (backward compatible approach)
```

**Implementation:**
```python
def validate_evaluation_metrics(metrics):
    """Support both old and new metric formats."""
    # Check for new format first
    new_format = [m for m in metrics if m['name'] == 'gen_ai.evaluation.score']
    if new_format:
        return validate_new_format(new_format)
    
    # Fall back to old format
    old_format = [m for m in metrics if 'gen_ai.evaluation.' in m['name']]
    if old_format:
        return validate_old_format(old_format)
    
    raise ValueError("No evaluation metrics found")
```

---

### 3.2 Blocker #2: Demo Apps Running Out of Tokens

**Status:** âœ… **RESOLVED**

**Issue:**
```
Nov 26, 2025:
"Demo apps are running out of tokens - need to switch to CircuIT"

Dec 1, 2025:
"Demo apps are running out of tokens - need to switch to CircuIT
Direct connection to circuit for evals, switching to self-hosted llama"
```

**Resolution:**
```
Action: Switched to Circuit for evaluations
Timeline: Completed Dec 19, 2025
Impact: Reduced token costs, improved reliability
```

---

### 3.3 Blocker #3: LiteLLM Proxy OTel Dependencies

**Status:** âœ… **RESOLVED**

**Issue:**
```
Dec 15, 2025:
"LiteLLM Proxy does not have proper requirements for 
opentelemetry-sdk dependencies"

"The above requirement potentially will conflict with 
zero-code instrumentation"
```

**Resolution:**
```
Action: 
1. Updated LiteLLM Proxy requirements
2. Disabled automatic zero-code instrumentation for LiteLLM Proxy
3. Manual instrumentation with shell script entrypoint

Timeline: Completed Dec 16, 2025
Impact: LiteLLM telemetry now working correctly
```

---

## 4. Test Plan Impact Assessment

### 4.1 Test Case Changes Summary

| Change Type | Original Count | New Count | Delta | Impact |
|-------------|---------------|-----------|-------|--------|
| **Session Tests** | 3 | 0 | -3 | Removed (PI3) |
| **Evaluation Tests** | 2 | 3 | +1 | Multi-judge |
| **Traceloop Tests** | 0 | 1 | +1 | New backend |
| **Trace Details Tests** | 0 | 1 | +1 | Span Store API |
| **Parity Tests** | 0 | 2 | +2 | Cross-app validation |
| **Total P0 Tests** | 33 | 30 | -3 | **FINAL COUNT** |

### 4.2 Detailed Test Case Mapping

#### **REMOVED Test Cases** âŒ

```
TC-PI2-SESSION-01: Session ID Propagation
â”œâ”€â”€ Status: REMOVED
â”œâ”€â”€ Reason: Session support deferred to PI3
â”œâ”€â”€ Effort Saved: 0.5 days
â””â”€â”€ Deferred To: PI3 (Q2 2026)

TC-PI2-SESSION-02: Session List View
â”œâ”€â”€ Status: REMOVED
â”œâ”€â”€ Reason: UI dependent on session backend
â”œâ”€â”€ Effort Saved: 1.0 days
â””â”€â”€ Deferred To: PI3 (Q2 2026)

TC-PI2-SESSION-03: Session Detail View
â”œâ”€â”€ Status: REMOVED
â”œâ”€â”€ Reason: UI dependent on session backend
â”œâ”€â”€ Effort Saved: 1.0 days
â””â”€â”€ Deferred To: PI3 (Q2 2026)

Total Effort Saved: 2.5 days
```

#### **ADDED Test Cases** âœ…

```
TC-PI2-EVAL-03: Circuit Judge Validation
â”œâ”€â”€ Status: NEW
â”œâ”€â”€ Reason: Multi-judge backend support
â”œâ”€â”€ Effort: 1.0 days
â”œâ”€â”€ Priority: P0
â””â”€â”€ Week: Week 3

TC-PI2-TRACELOOP-01: Traceloop Backend Validation
â”œâ”€â”€ Status: NEW
â”œâ”€â”€ Reason: Validate Traceloop translator
â”œâ”€â”€ Effort: 1.0 days
â”œâ”€â”€ Priority: P0
â””â”€â”€ Week: Week 4

TC-PI2-TRACE-DETAILS-01: Span Store API Migration
â”œâ”€â”€ Status: NEW
â”œâ”€â”€ Reason: Backend migration from logs
â”œâ”€â”€ Effort: 1.0 days
â”œâ”€â”€ Priority: P0
â””â”€â”€ Week: Week 2

TC-PI2-PARITY-01: App 2 Feature Parity
â”œâ”€â”€ Status: NEW
â”œâ”€â”€ Reason: Validate Traceloop parity
â”œâ”€â”€ Effort: 1.0 days
â”œâ”€â”€ Priority: P0
â””â”€â”€ Week: Week 4

TC-PI2-PARITY-02: App 3 Feature Parity
â”œâ”€â”€ Status: NEW
â”œâ”€â”€ Reason: Validate hybrid setup parity
â”œâ”€â”€ Effort: 0.5 days
â”œâ”€â”€ Priority: P1 â†’ P0
â””â”€â”€ Week: Week 5 (Optional)

Total Effort Added: 4.5 days
```

#### **UPDATED Test Cases** âš ï¸

```
TC-PI2-EVAL-02: Server-Side Evaluation Execution
â”œâ”€â”€ Status: UPDATED
â”œâ”€â”€ Change: Add Circuit judge comparison
â”œâ”€â”€ Additional Effort: +0.5 days
â””â”€â”€ Week: Week 3

TC-PI2-STREAMING-01: Streaming TTFT Validation
â”œâ”€â”€ Status: UPDATED
â”œâ”€â”€ Change: Validate TelemetryHandler async handling
â”œâ”€â”€ Additional Effort: +0.2 days
â””â”€â”€ Week: Week 2

TC-PI2-AIDEF-01, 02, 03: AI Defense Integration
â”œâ”€â”€ Status: UPDATED
â”œâ”€â”€ Change: Use multi-agent travel planner example
â”œâ”€â”€ Additional Effort: +0.3 days
â””â”€â”€ Week: Week 2-3

Total Additional Effort: 1.0 days
```

### 4.3 Net Effort Impact

```
Original Effort: 22 days (33 tests)
Effort Saved: -2.5 days (session tests removed)
Effort Added: +4.5 days (new tests)
Additional Effort: +1.0 days (updates)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Net Effort: 25 days (30 tests)

Impact: +3 days (+13.6% increase)
Mitigation: Reduced scope (30 vs 33 tests) offsets increase
```

---

## 5. Revised Test Strategy

### 5.1 Updated Test Application Strategy

**Original Strategy:**
```
APP 1: Foundation (14 tests) - Foundation + Session + LangGraph
APP 2: Evaluation (11 tests) - AI Defense + Platform Eval
APP 3: Azure (8 tests) - LiteLLM + RBAC + Streaming
```

**Revised Strategy:**
```
APP 1: Foundation (28 tests) - ALL FEATURES
â”œâ”€â”€ Foundation Components (5 tests)
â”œâ”€â”€ AI Defense (3 tests)
â”œâ”€â”€ Platform Evaluations (3 tests)
â”œâ”€â”€ LangGraph (1 test)
â”œâ”€â”€ RAG (1 test)
â”œâ”€â”€ LiteLLM (2 tests)
â”œâ”€â”€ Streaming (1 test)
â”œâ”€â”€ Cost (2 tests)
â”œâ”€â”€ RBAC (2 tests)
â”œâ”€â”€ Alerting (2 tests)
â”œâ”€â”€ UI (5 tests)
â””â”€â”€ Data/Multi-realm (2 tests)

APP 2: Traceloop (2 tests) - Backend Validation
â”œâ”€â”€ Traceloop Backend (1 test)
â””â”€â”€ Feature Parity (1 test)

APP 3: Hybrid (2 tests) - Optional
â”œâ”€â”€ Feature Parity (1 test)
â””â”€â”€ Edge Cases (1 test)

Total: 30 P0 tests (32 with optional)
```

### 5.2 Updated Weekly Breakdown

```
Week 1 (Dec 16-20): Setup âœ… COMPLETED
â”œâ”€â”€ Framework setup
â”œâ”€â”€ Test data preparation
â””â”€â”€ App deployment

Week 2 (Dec 23-27): App 1 Core âœ… COMPLETED
â”œâ”€â”€ Foundation (5 tests)
â”œâ”€â”€ LiteLLM (2 tests)
â”œâ”€â”€ AI Defense (2 tests)
â”œâ”€â”€ Trace Details (1 test)
â”œâ”€â”€ Streaming (1 test)
â””â”€â”€ Azure (1 test)
Total: 12 tests

Week 3 (Dec 30 - Jan 3): App 1 Complete âœ… COMPLETED
â”œâ”€â”€ Platform Evals (3 tests) â† Multi-judge added
â”œâ”€â”€ RAG (1 test)
â”œâ”€â”€ UI (5 tests)
â”œâ”€â”€ Alerting (2 tests)
â””â”€â”€ Cost (2 tests)
Total: 13 tests (11 + 2 from multi-judge)

Week 4 (Jan 6-10): Integration ğŸ”„ IN PROGRESS
â”œâ”€â”€ RBAC (2 tests)
â”œâ”€â”€ Traceloop (2 tests) â† NEW
â”œâ”€â”€ Multi-realm (1 test)
â””â”€â”€ Regression (44 tests)
Total: 5 new tests + regression

Week 5 (Jan 13-17): Final Validation â³ PLANNED
â”œâ”€â”€ App 3 parity (2 tests) â† Optional
â”œâ”€â”€ Final regression
â”œâ”€â”€ Performance validation
â””â”€â”€ GA sign-off (Jan 20)
Total: 2 optional tests
```

---

## 6. Updated Test Case Mapping

### 6.1 Complete Test Case List (30 P0 Tests)

| # | Test ID | Description | App | Week | Status | Change |
|---|---------|-------------|-----|------|--------|--------|
| 1 | TC-PI2-FOUNDATION-01 | Orchestrator Pattern | App 1 | 2 | âœ… | No change |
| 2 | TC-PI2-FOUNDATION-02 | Parallel Agents | App 1 | 2 | âœ… | No change |
| 3 | TC-PI2-FOUNDATION-03 | MCP Protocol | App 1 | 3 | âœ… | No change |
| 4 | TC-PI2-FOUNDATION-04 | Multi-Instrumentation | App 1 | 3 | âœ… | No change |
| 5 | TC-PI2-FOUNDATION-05 | RAG Pipeline | App 1 | 3 | âœ… | No change |
| 6 | TC-PI2-PLATFORM-05 | LangGraph Multi-Agent | App 1 | 2 | âœ… | No change |
| 7 | TC-PI2-RAG-01 | RAG E2E | App 1 | 3 | âœ… | No change |
| 8 | TC-PI2-LITELLM-01 | Proxy Metrics | App 1 | 2 | âœ… | Updated (OTel fix) |
| 9 | TC-PI2-LITELLM-02 | Trace Correlation | App 1 | 2 | âœ… | No change |
| 10 | TC-PI2-AIDEF-01 | AI Defense API | App 1 | 2 | âœ… | Updated (example) |
| 11 | TC-PI2-AIDEF-02 | AI Defense Proxy | App 1 | 2 | âœ… | Updated (example) |
| 12 | TC-PI2-AIDEF-03 | Risk UI Visibility | App 1 | 3 | âœ… | Updated (example) |
| 13 | TC-PI2-EVAL-01 | Platform Eval Config | App 1 | 3 | âœ… | No change |
| 14 | TC-PI2-EVAL-02 | OpenAI Judge | App 1 | 3 | âœ… | Updated (compare) |
| 15 | TC-PI2-EVAL-03 | Circuit Judge | App 1 | 3 | âœ… | **NEW** |
| 16 | TC-PI2-ALERT-01 | Alert Creation | App 1 | 3 | âœ… | No change |
| 17 | TC-PI2-ALERT-02 | Alert Triggering | App 1 | 3 | âœ… | No change |
| 18 | TC-PI2-COST-01 | Cost Calculation | App 1 | 3 | âœ… | No change |
| 19 | TC-PI2-COST-02 | Cost Display | App 1 | 4 | ğŸ”„ | No change |
| 20 | TC-PI2-RBAC-01 | Role Configuration | App 1 | 4 | ğŸ”„ | No change |
| 21 | TC-PI2-RBAC-03 | Viewer Access | App 1 | 4 | ğŸ”„ | No change |
| 22 | TC-PI2-STREAMING-01 | Streaming TTFT | App 1 | 2 | âœ… | Updated (async) |
| 23 | TC-PI2-TRACELOOP-01 | Traceloop Backend | App 2 | 4 | ğŸ”„ | **NEW** |
| 24 | TC-PI2-TRACE-DETAILS-01 | Span Store API | All | 2 | âœ… | **NEW** |
| 25 | TC-PI2-EXPLORER-01 | Interaction List | All | 3 | âœ… | No change |
| 26 | TC-PI2-PLATFORM-06 | Azure Provider | App 1 | 2 | âœ… | No change |
| 27 | TC-PI2-DATA-01 | Test Data | All | 1 | âœ… | No change |
| 28 | TC-PI2-02 | Multi-Realm | All | 4 | ğŸ”„ | No change |
| 29 | TC-PI2-PARITY-01 | App 2 Parity | App 2 | 4 | ğŸ”„ | **NEW** |
| 30 | TC-PI2-PARITY-02 | App 3 Parity | App 3 | 5 | â³ | **NEW** (Optional) |

**Legend:**
- âœ… Completed
- ğŸ”„ In Progress
- â³ Planned
- **NEW** = New test case
- Updated = Modified test case

### 6.2 Removed Test Cases (Deferred to PI3)

| # | Test ID | Description | Reason | Deferred To |
|---|---------|-------------|--------|-------------|
| âŒ | TC-PI2-SESSION-01 | Session ID Propagation | Semantic convention pending | PI3 Q2 2026 |
| âŒ | TC-PI2-SESSION-02 | Session List View | Backend not ready | PI3 Q2 2026 |
| âŒ | TC-PI2-SESSION-03 | Session Detail View | Backend not ready | PI3 Q2 2026 |

---

## 7. Risk Mitigation Plan

### 7.1 Updated Risk Assessment

| Risk | Original | Current | Mitigation | Status |
|------|----------|---------|------------|--------|
| **TIAA â†’ Lumenova** | 60% | 40% | App 1 100% by Week 3 | ğŸŸ¢ On Track |
| **Foundation Delays** | 40% | 20% | Daily sync, partial demo OK | ğŸŸ¢ Resolved |
| **Platform Eval Service** | 50% | 30% | Circuit fallback available | ğŸŸ¢ Mitigated |
| **Single QE Resource** | 90% | 70% | Reduced scope (30 vs 33) | ğŸŸ¡ Managed |
| **Span Store Migration** | 30% | 20% | Test with current API | ğŸŸ¢ Low Risk |
| **Session Support** | 70% | 0% | Removed from PI2 | âœ… Eliminated |

### 7.2 New Risks Identified

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| **Evaluation Metric Naming** | 40% | MEDIUM | Support both formats |
| **Multi-Judge Consistency** | 30% | LOW | Allow Â±10% variance |
| **Traceloop Backend** | 25% | LOW | Can test with existing |

---

## 8. Action Items

### 8.1 Immediate Actions (Week 4: Jan 6-10)

```
Priority 1: CRITICAL
â”œâ”€â”€ âœ… Remove session test cases from test plan
â”œâ”€â”€ âœ… Update test case count (33 â†’ 30)
â”œâ”€â”€ ğŸ”„ Update metric validators for new naming convention
â”œâ”€â”€ ğŸ”„ Integrate AI Defense multi-agent example
â””â”€â”€ ğŸ”„ Add Circuit judge test case

Priority 2: HIGH
â”œâ”€â”€ ğŸ”„ Complete RBAC tests (TC-PI2-RBAC-01, 03)
â”œâ”€â”€ ğŸ”„ Complete Traceloop validation (TC-PI2-TRACELOOP-01)
â”œâ”€â”€ ğŸ”„ Complete multi-realm testing (TC-PI2-02)
â””â”€â”€ ğŸ”„ Execute regression suite (44 tests)

Priority 3: MEDIUM
â”œâ”€â”€ â³ Update documentation with changes
â”œâ”€â”€ â³ Prepare Week 5 optional tests
â””â”€â”€ â³ Finalize GA sign-off checklist
```

### 8.2 Code Changes Required

```python
# 1. Update validators/metric_validator.py
def validate_evaluation_metrics(metrics):
    """Support both old and new metric naming."""
    # Implementation provided in Section 3.1

# 2. Update tests/e2e/test_platform_evaluations.py
@pytest.mark.p0
def test_circuit_judge_evaluation(config, apm_client):
    """TC-PI2-EVAL-03: Circuit judge validation."""
    # Implementation provided in Section 2.1

# 3. Add tests/integration/test_traceloop_backend.py
@pytest.mark.p0
def test_traceloop_backend_processing(apm_client):
    """TC-PI2-TRACELOOP-01: Traceloop backend validation."""
    # Implementation provided in Section 2.1

# 4. Update fixtures/app_fixtures.py
@pytest.fixture
def ai_defense_travel_planner():
    """Deploy AI Defense travel planner example."""
    # Implementation provided in Section 2.1
```

### 8.3 Documentation Updates

```
Documents to Update:
â”œâ”€â”€ âœ… GA_test_plan.md (Remove session tests, add new tests)
â”œâ”€â”€ âœ… GA_supplementry_doc.md (Update test case specs)
â”œâ”€â”€ ğŸ”„ Test execution schedule (Adjust weekly breakdown)
â”œâ”€â”€ ğŸ”„ Risk assessment (Update risk levels)
â””â”€â”€ ğŸ”„ Success criteria (Update metrics)
```

---

## 9. Timeline Adjustments

### 9.1 Original vs Revised Timeline

```
Original Timeline (Dec 18, 2025):
â”œâ”€â”€ Week 1: Setup (1 test)
â”œâ”€â”€ Week 2: Core (11 tests)
â”œâ”€â”€ Week 3: Platform (9 tests)
â”œâ”€â”€ Week 4: Integration (5 tests)
â””â”€â”€ Week 5: Final (7 tests + regression)
Total: 33 tests

Revised Timeline (Jan 12, 2026):
â”œâ”€â”€ Week 1: Setup (1 test) âœ… COMPLETED
â”œâ”€â”€ Week 2: Core (12 tests) âœ… COMPLETED
â”œâ”€â”€ Week 3: Platform (11 tests) âœ… COMPLETED
â”œâ”€â”€ Week 4: Integration (5 tests) ğŸ”„ IN PROGRESS
â””â”€â”€ Week 5: Final (1 test + regression) â³ PLANNED
Total: 30 tests

Net Change: -3 tests, +1 week buffer
```

### 9.2 Critical Path Analysis

```
Critical Path (Week 4-5):
â”œâ”€â”€ Day 1-2 (Jan 6-7): RBAC tests
â”œâ”€â”€ Day 3-4 (Jan 8-9): Traceloop + Multi-realm
â”œâ”€â”€ Day 5 (Jan 10): Regression suite
â”œâ”€â”€ Day 6-8 (Jan 13-15): Optional tests + fixes
â”œâ”€â”€ Day 9-10 (Jan 16-17): Final validation
â””â”€â”€ Day 11 (Jan 20): GA sign-off

Buffer: 3 days (Jan 17-20)
Risk: LOW (ahead of schedule)
```

---

## 10. Recommendations

### 10.1 Immediate Recommendations

1. âœ… **APPROVE** removal of session tests from PI2
   - Rationale: Semantic convention not ready
   - Impact: Reduces scope, improves timeline
   - Defer to: PI3 (Q2 2026)

2. âœ… **IMPLEMENT** dual metric naming support
   - Rationale: Smooth transition to new naming
   - Impact: Backward compatible
   - Timeline: Week 4 (Jan 6-10)

3. âœ… **INTEGRATE** AI Defense multi-agent example
   - Rationale: Production-ready reference
   - Impact: Improves test quality
   - Timeline: Week 4 (Jan 6-10)

4. âœ… **ADD** Circuit judge test case
   - Rationale: Multi-judge support is GA feature
   - Impact: +1 test case, +1 day effort
   - Timeline: Week 3 (completed)

### 10.2 Long-Term Recommendations

1. **PI3 Planning** (Q2 2026)
   - Add session tracking tests (3 tests)
   - Add CrewAI instrumentation tests (2 tests)
   - Add LlamaIndex RAG tests (2 tests)
   - Total: 7 additional tests

2. **Framework Enhancements**
   - Add performance testing capabilities
   - Integrate chaos engineering
   - Expand to EU0, JP0 realms

3. **Automation Improvements**
   - Increase automation from 67% to 80%
   - Add AI-powered test generation
   - Implement self-healing tests

---

## Conclusion

The January 2026 changes have resulted in a **net reduction of 3 test cases** (33 â†’ 30) while **improving test quality** through:

1. âœ… **Removal of unready features** (session tracking)
2. âœ… **Addition of production-ready features** (multi-judge, Traceloop)
3. âœ… **Integration of real examples** (AI Defense multi-agent)
4. âœ… **Improved metric naming** (standardization)

**Overall Impact:** ğŸŸ¢ **POSITIVE**
- Reduced scope improves timeline confidence
- New tests are smaller and more focused
- GA date remains achievable (Feb 25, 2026)
- Zero P0 bugs target maintained

**Recommendation:** âœ… **PROCEED** with revised test plan

---

**Document Owner:** Ankur Kumar Shandilya  
**Last Updated:** January 12, 2026  
**Next Review:** January 17, 2026 (Final validation)
**Approval Status:** â³ Pending stakeholder review
